const axios = require('axios');

class LlamaService {
  constructor() {
    this.ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
    this.model = process.env.OLLAMA_MODEL || 'llama3.2:3b';
    this.isAvailable = false;
    this.checkAvailability();
  }

  async checkAvailability() {
    try {
      const response = await axios.get(`${this.ollamaUrl}/api/tags`);
      this.isAvailable = true;
      console.log('‚úÖ Ollama est disponible');
      
      // V√©rifier si le mod√®le est install√©
      const models = response.data.models || [];
      const modelExists = models.some(m => m.name.includes(this.model.split(':')[0]));
      
      if (!modelExists) {
        console.log(`‚ö†Ô∏è  Mod√®le ${this.model} non trouv√©. Tentative de t√©l√©chargement...`);
        await this.pullModel();
      }
    } catch (error) {
      this.isAvailable = false;
      console.log('‚ùå Ollama non disponible:', error.message);
      console.log('üí° Pour installer Ollama: https://ollama.ai/download');
    }
  }

  async pullModel() {
    try {
      console.log(`üì• T√©l√©chargement du mod√®le ${this.model}...`);
      await axios.post(`${this.ollamaUrl}/api/pull`, {
        name: this.model
      });
      console.log(`‚úÖ Mod√®le ${this.model} t√©l√©charg√© avec succ√®s`);
    } catch (error) {
      console.error(`‚ùå Erreur lors du t√©l√©chargement du mod√®le:`, error.message);
    }
  }

  async generateResponse(messages, options = {}) {
    if (!this.isAvailable) {
      throw new Error('Ollama n\'est pas disponible. Veuillez l\'installer et le d√©marrer.');
    }

    try {
      // Convertir les messages au format Ollama
      const prompt = this.formatMessages(messages);
      
      const response = await axios.post(`${this.ollamaUrl}/api/generate`, {
        model: this.model,
        prompt: prompt,
        stream: false,
        options: {
          temperature: options.temperature || 0.7,
          top_p: options.top_p || 0.9,
          max_tokens: options.max_tokens || 500
        }
      });

      return response.data.response;
    } catch (error) {
      console.error('Erreur Ollama:', error.message);
      throw new Error(`Erreur lors de la g√©n√©ration de la r√©ponse: ${error.message}`);
    }
  }

  formatMessages(messages) {
    // Convertir les messages en format de prompt pour Ollama
    let prompt = '';
    
    messages.forEach(message => {
      if (message.role === 'system') {
        prompt += `Syst√®me: ${message.content}\n\n`;
      } else if (message.role === 'user') {
        prompt += `Utilisateur: ${message.content}\n\n`;
      } else if (message.role === 'assistant') {
        prompt += `Assistant: ${message.content}\n\n`;
      }
    });
    
    prompt += 'Assistant: ';
    return prompt;
  }

  // M√©thodes sp√©cialis√©es pour la gestion de t√¢ches
  async generateSubtasks(taskDescription) {
    const messages = [
      {
        role: 'system',
        content: 'Tu es un assistant sp√©cialis√© dans la gestion de t√¢ches. G√©n√®re une liste de sous-t√¢ches claires et actionables.'
      },
      {
        role: 'user',
        content: `G√©n√®re 3-5 sous-t√¢ches pour cette t√¢che: "${taskDescription}". R√©ponds uniquement avec une liste num√©rot√©e, sans texte suppl√©mentaire.`
      }
    ];

    return await this.generateResponse(messages, { max_tokens: 200 });
  }

  async rephraseTask(taskDescription) {
    const messages = [
      {
        role: 'system',
        content: 'Tu es un assistant qui aide √† reformuler les t√¢ches pour les rendre plus claires et actionables.'
      },
      {
        role: 'user',
        content: `Reformule cette t√¢che pour la rendre plus claire et pr√©cise: "${taskDescription}". R√©ponds uniquement avec la t√¢che reformul√©e.`
      }
    ];

    return await this.generateResponse(messages, { max_tokens: 100 });
  }

  async analyzeTask(taskDescription) {
    const messages = [
      {
        role: 'system',
        content: 'Tu es un expert en gestion de projet. Analyse les t√¢ches et fournis des recommandations sur la priorit√© et l\'estimation du temps.'
      },
      {
        role: 'user',
        content: `Analyse cette t√¢che et fournis: 1) Priorit√© recommand√©e (low/medium/high), 2) Estimation du temps en heures, 3) Br√®ve justification. T√¢che: "${taskDescription}"`
      }
    ];

    return await this.generateResponse(messages, { max_tokens: 150 });
  }

  async chatResponse(messages) {
    const systemMessage = {
      role: 'system',
      content: 'Tu es un assistant IA sp√©cialis√© dans la gestion de t√¢ches et de projets. Tu aides les utilisateurs √† organiser leur travail, planifier leurs projets et optimiser leur productivit√©. R√©ponds de mani√®re concise et utile.'
    };

    const fullMessages = [systemMessage, ...messages];
    return await this.generateResponse(fullMessages);
  }
}

module.exports = new LlamaService();